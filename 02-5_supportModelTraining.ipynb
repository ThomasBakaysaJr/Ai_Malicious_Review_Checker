{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef562cd8-dfb1-4ebf-8f46-8c50e7d68288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f3075d5-5083-4eda-8d8a-68b02251f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting zipping\n",
      "text       object\n",
      "rating    float64\n",
      "label       int64\n",
      "dtype: object\n",
      "Chunk 1 saved at files/review_chunks/zip_cleaned_data_01.csv\n",
      "Time elapsed: 0.3797435760498047 seconds\n",
      "Finished zipping\n",
      "Time elapsed: 0.37987256050109863 seconds\n"
     ]
    }
   ],
   "source": [
    "# converts to lowercase and strip punctuation\n",
    "def convertLine(text, is_review):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + '/xa0'))\n",
    "\n",
    "    if is_review:\n",
    "        text = json.dumps(text.split('\\t', 3))\n",
    "    else:\n",
    "        text = json.dumps(text.split('\\t'))\n",
    "    text = pd.read_json(StringIO(text), lines=True)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# going to read it in chunks\n",
    "chunk_size = 100000\n",
    "\n",
    "\n",
    "# training the fake reviews model to an acceptable accuracy.\n",
    "# load the file\n",
    "review_path = 'reviews/YelpZip/reviewContent'\n",
    "meta_path = 'reviews/YelpZip/metadata'\n",
    "final_path = 'files/review_chunks/zip_cleaned_data_0'\n",
    "\n",
    "# actual col re-names and the masks were going to use\n",
    "review_cols = {0: 'user_id', 1: 'blank', 2: 'date', 3: 'text'}\n",
    "meta_cols = {0: 'user_id', 1: 'blank', 2: 'rating', 3: 'label', 4: 'date'}\n",
    "review_col_mask = ['text']\n",
    "meta_col_mask = ['rating', 'label']\n",
    "\n",
    "with open(review_path, 'r', encoding='utf-8') as f_review:\n",
    "    with open(meta_path, 'r', encoding='utf-8') as f_meta:\n",
    "        chunk_r = []\n",
    "        chunk_m = []\n",
    "\n",
    "        chunk_r_df = pd.DataFrame(columns = review_cols)\n",
    "        chunk_m_df = pd.DataFrame(columns = meta_cols)\n",
    "        \n",
    "        print(f'Starting zipping')\n",
    "        t0 = time.time()\n",
    "        count = 1\n",
    "        \n",
    "        for index, (line_f, line_m) in enumerate(zip(f_review, f_meta)):\n",
    "            # read each line as a dataframe then append to a list\n",
    "            # review structure [index,\n",
    "            chunk_r.append(convertLine(line_f, True))\n",
    "            chunk_m.append(convertLine(line_m, False))\n",
    "\n",
    "            # save chunk to disk\n",
    "            if (index + 1) % chunk_size == 0:\n",
    "                chunk_r_df = pd.concat(chunk_r, ignore_index=True).rename(columns = review_cols)[review_col_mask]\n",
    "                chunk_m_df = pd.concat(chunk_m, ignore_index=True).rename(columns = meta_cols)[meta_col_mask]\n",
    "        \n",
    "                # remove the columns we don't care about and then concat them into the final data frame\n",
    "                # chunk_r_df = chunk_r_df[rwsub_less]\n",
    "                # chunk_m_df = chunk_m_df[]\n",
    "                final_df = pd.concat([chunk_r_df, chunk_m_df], axis=1)\n",
    "                final_df['rating'] = final_df['rating'] / 5.0\n",
    "                final_df.convert_dtypes()\n",
    "        \n",
    "                # write the cleaned and organized dataframe to a file\n",
    "                save_path = f'{final_path}{count}.csv'\n",
    "                final_df.to_csv(save_path, index=False)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f'Chunk {count} saved at {save_path}')\n",
    "                    print(f'Time elapsed: {time.time() - t0} seconds')\n",
    "                count += 1\n",
    "                break\n",
    "\n",
    "print('Finished zipping')\n",
    "if verbose:\n",
    "    print(f'Time elapsed: {time.time() - t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842429d-3e9b-4c2f-8f74-ae6d482f2aa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorize with tf-idf\n",
    "vizer = TfidfVectorizer()\n",
    "\n",
    "x_text = vizer.fit_transform(fakeDf['text_'])\n",
    "\n",
    "if verbose:\n",
    "    print(x_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31cd57-bf3e-4aaf-9347-d07679a248c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the sparse matrix with the dense ratings column\n",
    "\n",
    "# turn into 2d array\n",
    "rate_feature = fakeDf['rating'].values.reshape(-1, 1)\n",
    "\n",
    "# combine vectorized text and ratings\n",
    "# data\n",
    "X = hstack([x_text, rate_feature])\n",
    "\n",
    "# target labels\n",
    "y = fakeDf['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120425e4-6867-40b9-97fb-905f4b875440",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "c_params = [1.0, 0.7, 0.5, 0.3]\n",
    "clf_names = []\n",
    "\n",
    "# train the thing || trying differnt C score on SVM and Logistic Regression\n",
    "models = {}\n",
    "\n",
    "for c in c_params:\n",
    "    # create a svm models with this c value\n",
    "    name = 'clf_svm' + str(c)\n",
    "    clf_names.append(name)\n",
    "    models[name] = LinearSVC(C=c)\n",
    "\n",
    "    t0 = time.time()\n",
    "    if verbose:\n",
    "        print(f'Training {name}')\n",
    "    # actual training\n",
    "    models[name].fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print(f'finished after {time.time() - t0} seconds')\n",
    "\n",
    "    # create a logistic regression models with this c value\n",
    "    name = 'clf_log' + str(c)\n",
    "    clf_names.append(name)\n",
    "    models[name] = LogisticRegression(C=c, max_iter=1000)\n",
    "\n",
    "    t0 = time.time()\n",
    "    if verbose:\n",
    "        print(f'Training {name}')\n",
    "    # actual training\n",
    "    models[name].fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print(f'finished after {time.time() - t0} seconds')\n",
    "\n",
    "# train a single random forest classifier\n",
    "name = 'clf_for'\n",
    "clf_names.append(name)\n",
    "t0 = time.time()\n",
    "if verbose:\n",
    "    print(f'Training {name}')\n",
    "models[name] = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "models[name].fit(X_train, y_train)\n",
    "if verbose:\n",
    "    print(f'finished after {time.time() - t0} seconds')\n",
    "\n",
    "if verbose:\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf43cd3-5cdd-4142-96dc-9c115aef02df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in clf_names:\n",
    "    y_pred = models[name].predict(X_test)\n",
    "    print(f'Classification report for {name}')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc89f5c-8bf9-4971-9cd3-34c68d5a07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing really beat the SVM with C=1.0\n",
    "# going to save that one and the vectorized (tf-idf)\n",
    "# important because the model trained on this\n",
    "# any new words might give me garbage.\n",
    "joblib.dump(models['clf_svm1.0'], 'models/support_svm.pkl')\n",
    "joblib.dump(models['clf_log1.0'], 'models/support_log.pkl')\n",
    "joblib.dump(models['clf_for'], 'models/support_for.pkl')\n",
    "joblib.dump(vizer, 'models/support_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42fa3e2-17bb-4d19-a208-7f39d6778d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac6491-ace6-4a73-8a44-02b0431bac9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
