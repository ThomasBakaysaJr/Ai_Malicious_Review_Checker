{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabef8ca-41ba-403c-9bb5-e716aab2b398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef562cd8-dfb1-4ebf-8f46-8c50e7d68288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from IPython.display import display\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3075d5-5083-4eda-8d8a-68b02251f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial columns are Index(['category', 'rating', 'label', 'text_'], dtype='object')\n",
      "\n",
      "Current dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>love this  well made sturdy and very comfortab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>love it a great upgrade from the original  ive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>this pillow saved my back i love the look and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>missing information on how to use it but it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>very nice set good quality we have had the set...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  label                                              text_\n",
       "0     1.0      1  love this  well made sturdy and very comfortab...\n",
       "1     1.0      1  love it a great upgrade from the original  ive...\n",
       "2     1.0      1  this pillow saved my back i love the look and ...\n",
       "3     0.2      1  missing information on how to use it but it is...\n",
       "4     1.0      1  very nice set good quality we have had the set..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# converts to lowercase and strip punctuation\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "\n",
    "# training the fake reviews model to an acceptable accuracy.\n",
    "# load the file\n",
    "fakeDf = pd.read_csv('reviews/fakeReviews/fakeReviews.csv')\n",
    "if verbose:\n",
    "    print(f'Initial columns are {fakeDf.columns}')\n",
    "\n",
    "# some preprocessing\n",
    "\n",
    "# convert labels to binary\n",
    "# fake will be 1 (the target we're looking for)\n",
    "fakeDf['label'] = fakeDf['label'].replace(['CG', 'OR'], [1, 0])\n",
    "\n",
    "\n",
    "# convert text to lower case and strip punctuation\n",
    "fakeDf['text_'] = fakeDf['text_'].apply(clean_text)\n",
    "\n",
    "# normalize ratings\n",
    "fakeDf['rating'] = fakeDf['rating'] / 5.0\n",
    "\n",
    "# remove category (not relevant for the yelp dataset, mismatch.)\n",
    "fakeDf.drop('category', inplace=True, axis=1)\n",
    "\n",
    "fakeDf = fakeDf.convert_dtypes()\n",
    "\n",
    "if verbose:\n",
    "    print('\\nCurrent dataframe')\n",
    "    display(fakeDf.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4842429d-3e9b-4c2f-8f74-ae6d482f2aa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40432, 51256)\n"
     ]
    }
   ],
   "source": [
    "# vectorize with tf-idf\n",
    "vizer = TfidfVectorizer()\n",
    "\n",
    "x_text = vizer.fit_transform(fakeDf['text_'])\n",
    "\n",
    "if verbose:\n",
    "    print(x_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d31cd57-bf3e-4aaf-9347-d07679a248c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the sparse matrix with the dense ratings column\n",
    "\n",
    "# turn into 2d array\n",
    "rate_feature = fakeDf['rating'].values.reshape(-1, 1)\n",
    "\n",
    "# combine ratings and vectorized text\n",
    "# data\n",
    "X = hstack([x_text, rate_feature])\n",
    "\n",
    "# target labels\n",
    "y = fakeDf['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "120425e4-6867-40b9-97fb-905f4b875440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_svm1.0\n",
      "finished after 0.3695993423461914 seconds\n",
      "Training clf_log1.0\n",
      "finished after 6.323486566543579 seconds\n",
      "Training clf_svm0.7\n",
      "finished after 0.38790392875671387 seconds\n",
      "Training clf_log0.7\n",
      "finished after 6.992849111557007 seconds\n",
      "Training clf_svm0.5\n",
      "finished after 0.2799110412597656 seconds\n",
      "Training clf_log0.5\n",
      "finished after 3.5008816719055176 seconds\n",
      "Training clf_svm0.3\n",
      "finished after 0.23955798149108887 seconds\n",
      "Training clf_log0.3\n",
      "finished after 2.9071192741394043 seconds\n",
      "Training clf_for\n",
      "finished after 84.16626000404358 seconds\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "c_params = [1.0, 0.7, 0.5, 0.3]\n",
    "clf_names = []\n",
    "\n",
    "# train the thing || trying differnt C score on SVM and Logistic Regression\n",
    "models = {}\n",
    "\n",
    "for c in c_params:\n",
    "    # create a svm models with this c value\n",
    "    name = 'clf_svm' + str(c)\n",
    "    clf_names.append(name)\n",
    "    models[name] = LinearSVC(C = c)\n",
    "                            \n",
    "    t0 = time.time()\n",
    "    if verbose:\n",
    "        print(f'Training {name}')\n",
    "    # actual training\n",
    "    models[name].fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print(f'finished after {time.time() - t0} seconds')\n",
    "        \n",
    "    #create a logistic regression models with this c value\n",
    "    name = 'clf_log' + str(c)\n",
    "    clf_names.append(name)\n",
    "    models[name] = LogisticRegression(C = c, max_iter=1000)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    if verbose:\n",
    "        print(f'Training {name}')\n",
    "    # actual training\n",
    "    models[name].fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print(f'finished after {time.time() - t0} seconds')\n",
    "\n",
    "# train a single random forest classifier\n",
    "name = 'clf_for'\n",
    "clf_names.append(name)\n",
    "t0 = time.time()\n",
    "if verbose:\n",
    "    print(f'Training {name}')\n",
    "models[name] = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "models[name].fit(X_train, y_train)\n",
    "if verbose:\n",
    "    print(f'finished after {time.time() - t0} seconds')\n",
    "        \n",
    "if verbose:\n",
    "    print('Finished training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcf43cd3-5cdd-4142-96dc-9c115aef02df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for clf_svm1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.91      0.91      4071\n",
      "         1.0       0.91      0.91      0.91      4016\n",
      "\n",
      "    accuracy                           0.91      8087\n",
      "   macro avg       0.91      0.91      0.91      8087\n",
      "weighted avg       0.91      0.91      0.91      8087\n",
      "\n",
      "Classification report for clf_log1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.92      0.91      4071\n",
      "         1.0       0.92      0.89      0.90      4016\n",
      "\n",
      "    accuracy                           0.90      8087\n",
      "   macro avg       0.90      0.90      0.90      8087\n",
      "weighted avg       0.90      0.90      0.90      8087\n",
      "\n",
      "Classification report for clf_svm0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.91      0.91      4071\n",
      "         1.0       0.91      0.91      0.91      4016\n",
      "\n",
      "    accuracy                           0.91      8087\n",
      "   macro avg       0.91      0.91      0.91      8087\n",
      "weighted avg       0.91      0.91      0.91      8087\n",
      "\n",
      "Classification report for clf_log0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.92      0.90      4071\n",
      "         1.0       0.91      0.88      0.90      4016\n",
      "\n",
      "    accuracy                           0.90      8087\n",
      "   macro avg       0.90      0.90      0.90      8087\n",
      "weighted avg       0.90      0.90      0.90      8087\n",
      "\n",
      "Classification report for clf_svm0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.91      0.91      4071\n",
      "         1.0       0.91      0.91      0.91      4016\n",
      "\n",
      "    accuracy                           0.91      8087\n",
      "   macro avg       0.91      0.91      0.91      8087\n",
      "weighted avg       0.91      0.91      0.91      8087\n",
      "\n",
      "Classification report for clf_log0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.92      0.90      4071\n",
      "         1.0       0.91      0.87      0.89      4016\n",
      "\n",
      "    accuracy                           0.90      8087\n",
      "   macro avg       0.90      0.90      0.90      8087\n",
      "weighted avg       0.90      0.90      0.90      8087\n",
      "\n",
      "Classification report for clf_svm0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.92      0.91      4071\n",
      "         1.0       0.92      0.90      0.91      4016\n",
      "\n",
      "    accuracy                           0.91      8087\n",
      "   macro avg       0.91      0.91      0.91      8087\n",
      "weighted avg       0.91      0.91      0.91      8087\n",
      "\n",
      "Classification report for clf_log0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.92      0.89      4071\n",
      "         1.0       0.91      0.86      0.89      4016\n",
      "\n",
      "    accuracy                           0.89      8087\n",
      "   macro avg       0.89      0.89      0.89      8087\n",
      "weighted avg       0.89      0.89      0.89      8087\n",
      "\n",
      "Classification report for clf_for\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.88      0.89      4071\n",
      "         1.0       0.88      0.90      0.89      4016\n",
      "\n",
      "    accuracy                           0.89      8087\n",
      "   macro avg       0.89      0.89      0.89      8087\n",
      "weighted avg       0.89      0.89      0.89      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in clf_names:\n",
    "    y_pred = models[name].predict(X_test)\n",
    "    print(f'Classification report for {name}')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dc89f5c-8bf9-4971-9cd3-34c68d5a07fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/support_vectorizer.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nothing really beat the SVM with C=1.0\n",
    "# going to save that one and the vectorized (tf-idf)\n",
    "# important because the model trained on this\n",
    "# any new words might give me garbage.\n",
    "joblib.dump(models['clf_svm1.0'],'models/support_svm.pkl')\n",
    "joblib.dump(vizer, 'models/support_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94167d32-19b1-4330-b3cf-43b6f55c95d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
