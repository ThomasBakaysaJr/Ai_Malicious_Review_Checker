{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef562cd8-dfb1-4ebf-8f46-8c50e7d68288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import time\n",
    "import gc\n",
    "import glob\n",
    "from io import StringIO\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb56a8a-0bf7-498a-b2c6-a85d5c7c3f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load combined and cleaned YelpZip file\n",
      "File not found. Trying to load cleaned file chunks.\n",
      "Files combined and loaded into zip_df.\n"
     ]
    }
   ],
   "source": [
    "zip_df_path = 'files/zip_cleaned_data_FULL.csv'\n",
    "zip_df_chunks_path = 'files/review_chunks/zip_cleaned_data_0*.csv'\n",
    "\n",
    "# Try stuff so I don't just keep getting errors if something goes wrong\n",
    "try:\n",
    "    print('Trying to load combined and cleaned YelpZip file')\n",
    "    zip_df = pd.read_csv(zip_df_path)\n",
    "    print('File found. File loaded into zip_df')\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        print('File not found. Trying to load cleaned file chunks.')\n",
    "        chunk_files = sorted(glob.glob(zip_df_chunks_path))\n",
    "        zip_df = pd.concat([pd.read_csv(file) for file in chunk_files], ignore_index = True)\n",
    "        zip_df.to_csv('files/zip_cleaned_data_FULL.csv')\n",
    "        print('Files combined and loaded into zip_df.')\n",
    "    except Exception as e:\n",
    "        print(f'Loading chunks failed.\\nError: {e}')\n",
    "except Exception as e:\n",
    "    print(r'Something weird happened ¯\\_(ツ)_/¯')\n",
    "    print(f'The weird thing is: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5529b01e-ffe4-4cad-b7dc-ffd4e419b11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600000, 3)\n",
      "label\n",
      " 1    520528\n",
      "-1     79472\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(zip_df.shape)\n",
    "print(zip_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4842429d-3e9b-4c2f-8f74-ae6d482f2aa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600000, 358661)\n"
     ]
    }
   ],
   "source": [
    "# structure of the dataframe should be [text, rating, label]\n",
    "# vectorize with tf-idf\n",
    "vizer = TfidfVectorizer()\n",
    "\n",
    "x_text = vizer.fit_transform(zip_df['text'])\n",
    "\n",
    "if verbose:\n",
    "    print(f'Dataframe shape: {zip_df.shape}')\n",
    "    print(f'Vectorized text shape: {x_text.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d31cd57-bf3e-4aaf-9347-d07679a248c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the sparse matrix with the dense ratings column\n",
    "\n",
    "# turn into 2d array\n",
    "rate_feature = zip_df['rating'].values.reshape(-1, 1)\n",
    "\n",
    "# combine vectorized text and ratings\n",
    "# data\n",
    "X = hstack([x_text, rate_feature])\n",
    "\n",
    "# target labels\n",
    "y = zip_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "120425e4-6867-40b9-97fb-905f4b875440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clf_log0.05\n",
      "finished after 7.183547258377075 seconds\n",
      "Training clf_log0.1\n",
      "finished after 13.426305294036865 seconds\n",
      "Training clf_log0.2\n",
      "finished after 20.486284732818604 seconds\n",
      "Training clf_log0.5\n",
      "finished after 41.79753518104553 seconds\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "c_params = [0.05, 0.1, 0.2, 0.5]\n",
    "clf_names = []\n",
    "\n",
    "# train the thing || trying differnt C score on SVM and Logistic Regression\n",
    "models = {}\n",
    "\n",
    "for c in c_params:\n",
    "    # create a svm model with this c value\n",
    "    # name = 'clf_svm' + str(c)\n",
    "    # clf_names.append(name)\n",
    "    # models[name] = LinearSVC(C=c)\n",
    "\n",
    "    # t0 = time.time()\n",
    "    # if verbose:\n",
    "    #     print(f'Training {name}')\n",
    "    # # actual training\n",
    "    # models[name].fit(X_train, y_train)\n",
    "    # if verbose:\n",
    "    #     print(f'finished after {time.time() - t0} seconds')\n",
    "\n",
    "    # create a logistic regression model with this c value\n",
    "    name = 'clf_log' + str(c)\n",
    "    clf_names.append(name)\n",
    "    models[name] = LogisticRegression(C=c, max_iter=1000, class_weight='balanced')\n",
    "\n",
    "    t0 = time.time()\n",
    "    if verbose:\n",
    "        print(f'Training {name}')\n",
    "    # actual training\n",
    "    models[name].fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print(f'finished after {time.time() - t0} seconds')\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf43cd3-5cdd-4142-96dc-9c115aef02df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for clf_log0.05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.69      0.36     15969\n",
      "           1       0.93      0.67      0.78    104031\n",
      "\n",
      "    accuracy                           0.68    120000\n",
      "   macro avg       0.59      0.68      0.57    120000\n",
      "weighted avg       0.84      0.68      0.73    120000\n",
      "\n",
      "Classification report for clf_log0.1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.69      0.36     15969\n",
      "           1       0.93      0.68      0.79    104031\n",
      "\n",
      "    accuracy                           0.68    120000\n",
      "   macro avg       0.59      0.68      0.58    120000\n",
      "weighted avg       0.84      0.68      0.73    120000\n",
      "\n",
      "Classification report for clf_log0.2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.68      0.37     15969\n",
      "           1       0.93      0.69      0.79    104031\n",
      "\n",
      "    accuracy                           0.69    120000\n",
      "   macro avg       0.59      0.68      0.58    120000\n",
      "weighted avg       0.84      0.69      0.73    120000\n",
      "\n",
      "Classification report for clf_log0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.67      0.37     15969\n",
      "           1       0.93      0.70      0.80    104031\n",
      "\n",
      "    accuracy                           0.69    120000\n",
      "   macro avg       0.59      0.68      0.58    120000\n",
      "weighted avg       0.84      0.69      0.74    120000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in clf_names:\n",
    "    y_pred = models[name].predict(X_test)\n",
    "    print(f'Classification report for {name}')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc89f5c-8bf9-4971-9cd3-34c68d5a07fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/support_vectorizer.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model for finding negative reviews is the logistic regression with c=0.1\n",
    "# going to save that one and the vectorized (tf-idf)\n",
    "# important because the model trained on this\n",
    "# any new words might give me garbage.\n",
    "\n",
    "# had tried svm's, they were bad at recalling the negative reviews.\n",
    "# got higher accurary mainly by labeling everything real\n",
    "joblib.dump(models['clf_log0.1'], 'models/support_log.pkl')\n",
    "joblib.dump(vizer, 'models/support_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42fa3e2-17bb-4d19-a208-7f39d6778d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac6491-ace6-4a73-8a44-02b0431bac9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
