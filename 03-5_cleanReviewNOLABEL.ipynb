{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca8304c-93cb-4cb1-873b-7d804543992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get overwrite files, just remove the .mark files\n",
    "\n",
    "# This is specifically for cleaning reviews for predicting ratings\n",
    "# going to clean the reviews here.\n",
    "import ast\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import autopep8\n",
    "import joblib\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy.sparse import hstack\n",
    "from io import StringIO\n",
    "\n",
    "# do get rid of annoying warnings\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8862ce46-fcb1-4da2-9c7c-21a38f524c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global, for the print statements\n",
    "verbose = True\n",
    "# set to True so that the notebook tries smaller chunks and only does 5 chunks\n",
    "test = False\n",
    "# chunk_size (will only use this if test if False)\n",
    "CHUNK_SIZE = 200000\n",
    "\n",
    "chunk_save_path = f\"files/review_chunks/no_label/rating_group/review_chunk\"\n",
    "# mark's only purpose is to inform the loop that this file exists or not\n",
    "mark_path = f\"files/review_chunks/no_label/rating_group/mark_{'_test' if test else ''}_0\"\n",
    "# starting at 0 since ML models like 0 indexed variables\n",
    "star_values = [0,1,2,3,4]\n",
    "\n",
    "rwpath = 'reviews/yelpReviews/yelp_academic_dataset_review.json'\n",
    "# cols we want from the reviews\n",
    "rwsub = ['user_id', 'business_id', 'text', 'date', 'stars']\n",
    "star_dist_path = 'files/star_dist.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e40587d-71b2-4883-9ffd-8576b4bf13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts to lowercase and strip punctuation\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def print_star_dist(star_values):\n",
    "    total = 0\n",
    "    total = sum(star_values)\n",
    "    print(f'Total is {total}')\n",
    "    for i, star in enumerate(star_values):\n",
    "        percent = 0 if total == 0 else star / total\n",
    "        print(f\"Star_{i}: {(percent * 100):.2f}%\\t count = {star}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85b822ed-1268-47ae-a771-cde00c8f65ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting labeling\n",
      "Chunks 1 already exits. Skipping\n",
      "Chunks 2 already exits. Skipping\n",
      "Chunks 3 already exits. Skipping\n",
      "Chunks 4 already exits. Skipping\n",
      "Chunks 5 already exits. Skipping\n",
      "TEST RUN\n",
      "Finished chunking reviews after 5.239248275756836e-05 minutes. Files are seperated into chunks of 200 lines.\n",
      "Distribution of stars is as follows\n",
      "Total is 1000\n",
      "Star_0: 11.20%\t count = 112\n",
      "Star_1: 7.40%\t count = 74\n",
      "Star_2: 12.60%\t count = 126\n",
      "Star_3: 22.80%\t count = 228\n",
      "Star_4: 46.00%\t count = 460\n"
     ]
    }
   ],
   "source": [
    "# going to have read the json file in chunks, the thing is almost 5 gigs\n",
    "chunk_size = 200 if test else CHUNK_SIZE\n",
    "\n",
    "with open(rwpath, 'r', encoding='utf-8') as file:\n",
    "    # create a chunk for each star, we will be saving these grouped together\n",
    "    chunk_dic = {}\n",
    "    for value in star_values:\n",
    "        chunk_dic[value] = []\n",
    "    chunk_count = []\n",
    "    count = 1\n",
    "    # to keep track of the distribution of stars\n",
    "    # so that I can keep that stratification during during\n",
    "    star_count = [0,0,0,0,0]\n",
    "\n",
    "    print(f'Starting labeling')\n",
    "    t0 = time.time()\n",
    "\n",
    "    # check if this chunk exists already (for restarts)\n",
    "    # will also load the star_count distribution.\n",
    "    check_path = f'{mark_path}{count}.mark' \n",
    "    if os.path.exists(check_path):\n",
    "        chunk_count.append(count)\n",
    "        # will only load here, since if this is skipped we're starting from scratch anyway\n",
    "        if os.path.exists(star_dist_path):\n",
    "            star_count = np.load(star_dist_path) \n",
    "        else:\n",
    "            print(f'!!!WARNING: Mark file exists but star_dist does not. Delete mark files to get proper star distribution.!!!')\n",
    "            sys.exit()\n",
    "    \n",
    "    for index, line in enumerate(file):\n",
    "        # only load and save the line if this chunk hasn't been done yet\n",
    "        if count not in chunk_count:\n",
    "            # read each line as a dataframe then append to a list\n",
    "            data = json.loads(line)\n",
    "            #data = pd.read_json(StringIO(line), lines = True)\n",
    "            # determine which dataframe to put this json object in\n",
    "            # star = star - 1 to make it zero indexed\n",
    "            star = data['stars'] = int(data['stars'])\n",
    "            chunk_dic[star - 1].append(data)\n",
    "        \n",
    "        # check if chunk is full \n",
    "        if (index + 1) % chunk_size == 0:\n",
    "            if count in chunk_count:\n",
    "                print(f'Chunks {count} already exits. Skipping')\n",
    "            else:\n",
    "                # convert json object lists into dataframes. remove\n",
    "                # columns we don't care about. The text also gets cleaned\n",
    "                for star in star_values:\n",
    "                    data = chunk_dic[star]\n",
    "                    data_df = pd.DataFrame(data) # convert to df\n",
    "                    data_df['text'] = data_df['text'].apply(clean_text) # clean text\n",
    "                    data_df['stars'] = data_df['stars'] - 1 # change so stars are z\n",
    "                    data_df = data_df[rwsub] # only keep relevant cols\n",
    "                    star_count[star] += len(data_df)\n",
    "                    data_df.to_csv(f\"{chunk_save_path}_star{star}_0{count}.csv\", index = False)\n",
    "                    np.save(star_dist_path, star_count)\n",
    "                    \n",
    "                    del data_df, data, chunk_dic[star]\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f'Chunk {count} for star value {star} saved.')\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'chunk {count} finished at {time.time() - t0} seconds.\\n')\n",
    "\n",
    "            # mark this as finished by saving the mark\n",
    "            mark_save_path = f\"{mark_path}{count}.mark\"\n",
    "            with open(mark_save_path, 'w') as f:\n",
    "                pass\n",
    "            \n",
    "            # clear dictionary for next loop\n",
    "            chunk_dic.clear()\n",
    "            for value in star_values:\n",
    "                chunk_dic[value] = []\n",
    "            \n",
    "            if count not in chunk_count:\n",
    "                chunk_count.append(count)\n",
    "            count += 1\n",
    "\n",
    "            mark_save_path = f\"{mark_path}{count}.mark\"\n",
    "            if os.path.exists(mark_save_path):\n",
    "                chunk_count.append(count)\n",
    "            \n",
    "            if test and count > 5:\n",
    "                break\n",
    "                \n",
    "np.save(star_dist_path, star_count)\n",
    "if test:\n",
    "    print('TEST RUN')\n",
    "print(f'Finished chunking reviews after {(time.time() - t0) / 60.0} minutes. Files are seperated into chunks of {chunk_size} lines.')\n",
    "print(f'\\nDistribution of stars is as follows')\n",
    "print_star_dist(star_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1d189-a45f-4931-823f-031cb71f2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finished chunking reviews after 0.016843199729919434 minutes. Files are seperated into chunks of 200 lines.\n",
    "Distribution of stars is as follows\n",
    "Star_0: 0% count = 112\n",
    "Star_1: 0% count = 74\n",
    "Star_2: 0% count = 126\n",
    "Star_3: 0% count = 228\n",
    "Star_4: 0% count = 460\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb02ea-8078-4953-8fb1-839f11eb3614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
