{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef562cd8-dfb1-4ebf-8f46-8c50e7d68288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import time\n",
    "import gc\n",
    "from io import StringIO\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89d3665f-7201-4a5f-a96c-fd124f745e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "# set to true to only write 3 chunks at 100 lines each, otherwise it will run it for the entire dataset\n",
    "# for testing\n",
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f3075d5-5083-4eda-8d8a-68b02251f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting zipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 saved at files/review_chunks/zip_cleaned_data_01.csv\n",
      "Time elapsed: 343.1141037940979 seconds\n",
      "Chunk 2 saved at files/review_chunks/zip_cleaned_data_02.csv\n",
      "Time elapsed: 693.2992417812347 seconds\n",
      "Chunk 3 saved at files/review_chunks/zip_cleaned_data_03.csv\n",
      "Time elapsed: 1039.409836769104 seconds\n",
      "Chunk 4 saved at files/review_chunks/zip_cleaned_data_04.csv\n",
      "Time elapsed: 1386.240077972412 seconds\n",
      "Chunk 5 saved at files/review_chunks/zip_cleaned_data_05.csv\n",
      "Time elapsed: 1758.5408165454865 seconds\n",
      "Chunk 6 saved at files/review_chunks/zip_cleaned_data_06.csv\n",
      "Time elapsed: 2143.4886162281036 seconds\n",
      "Finished zipping\n",
      "Time elapsed: 2174.5320341587067 seconds\n"
     ]
    }
   ],
   "source": [
    "# converts to lowercase and strip punctuation\n",
    "def convertLine(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    text = json.dumps(text.split('\\t', 3))\n",
    "    text = pd.read_json(StringIO(text), lines=True)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# going to read it in chunks\n",
    "chunk_size = 100 if test else 100000\n",
    "\n",
    "\n",
    "# training the fake reviews model to an acceptable accuracy.\n",
    "# load the file\n",
    "review_path = 'reviews/YelpZip/reviewContent'\n",
    "meta_path = 'reviews/YelpZip/metadata'\n",
    "final_path = 'files/review_chunks/zip_cleaned_data_0'\n",
    "\n",
    "# actual col re-names and the masks were going to use\n",
    "review_cols = {0: 'user_id', 1: 'blank', 2: 'date', 3: 'text'}\n",
    "meta_cols = {0: 'user_id', 1: 'blank', 2: 'rating', 3: 'label', 4: 'date'}\n",
    "review_col_mask = ['text']\n",
    "meta_col_mask = ['rating', 'label']\n",
    "\n",
    "with open(review_path, 'r', encoding='utf-8') as f_review:\n",
    "    with open(meta_path, 'r', encoding='utf-8') as f_meta:\n",
    "        \n",
    "        print(f'Starting zipping')\n",
    "        t0 = time.time()\n",
    "        count = 1\n",
    "        chunk_r = []\n",
    "        chunk_m = []\n",
    "\n",
    "        for index, (line_f, line_m) in enumerate(zip(f_review, f_meta)):\n",
    "            \n",
    "            # read each line as a dataframe then append to a list\n",
    "            processed_line = convertLine(line_f)\n",
    "            chunk_r.append(processed_line)\n",
    "\n",
    "            processed_line = json.dumps(line_m.split('\\t'))\n",
    "            chunk_m.append(pd.read_json(StringIO(processed_line), lines=True))\n",
    "            \n",
    "            # save chunk to disk\n",
    "            if (index + 1) % chunk_size == 0:\n",
    "                chunk_r_df = pd.concat(chunk_r, ignore_index=True).rename(columns = review_cols)[review_col_mask]\n",
    "                chunk_m_df = pd.concat(chunk_m, ignore_index=True).rename(columns = meta_cols)[meta_col_mask]\n",
    "        \n",
    "                # remove the columns we don't care about and then concat them into the final data frame\n",
    "                # chunk_r_df = chunk_r_df[rwsub_less]\n",
    "                # chunk_m_df = chunk_m_df[]\n",
    "                final_df = pd.concat([chunk_r_df, chunk_m_df], axis=1)\n",
    "                final_df['rating'] = final_df['rating'] / 5.0\n",
    "\n",
    "                # write the cleaned and organized dataframe to a file\n",
    "                save_path = f'{final_path}{count}.csv'\n",
    "                final_df.to_csv(save_path, index=False)\n",
    "                \n",
    "                # clear out garbage\n",
    "                del chunk_r, chunk_m, chunk_r_df, chunk_m_df, final_df\n",
    "                gc.collect()\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f'Chunk {count} saved at {save_path}')\n",
    "                    print(f'Time elapsed: {time.time() - t0} seconds')\n",
    "                count += 1\n",
    "\n",
    "                chunk_r = []\n",
    "                chunk_m = []\n",
    "\n",
    "                if test and count > 3:\n",
    "                    break\n",
    "\n",
    "print('Finished zipping')\n",
    "if verbose:\n",
    "    print(f'Time elapsed: {time.time() - t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ef0d7-3dc4-40ab-acdd-584a2dbdf9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
