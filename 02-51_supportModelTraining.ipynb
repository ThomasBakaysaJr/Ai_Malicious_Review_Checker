{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef562cd8-dfb1-4ebf-8f46-8c50e7d68288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import time\n",
    "import gc\n",
    "from io import StringIO\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d3665f-7201-4a5f-a96c-fd124f745e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "# set to true to only write 3 chunks at 100 lines each, otherwise it will run it for the entire dataset\n",
    "# for testing\n",
    "test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f3075d5-5083-4eda-8d8a-68b02251f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting zipping\n",
      "Chunk 1 saved at files/review_chunks/zip_cleaned_data_01.csv\n",
      "Time elapsed: 0.4526481628417969 seconds\n",
      "Chunk 2 saved at files/review_chunks/zip_cleaned_data_02.csv\n",
      "Time elapsed: 0.8368091583251953 seconds\n",
      "Chunk 3 saved at files/review_chunks/zip_cleaned_data_03.csv\n",
      "Time elapsed: 1.223862886428833 seconds\n",
      "Finished zipping\n",
      "Time elapsed: 1.2240910530090332 seconds\n"
     ]
    }
   ],
   "source": [
    "# converts to lowercase and strip punctuation\n",
    "def convertLine(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    text = json.dumps(text.split('\\t', 3))\n",
    "    text = pd.read_json(StringIO(text), lines=True)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# going to read it in chunks\n",
    "chunk_size = 100 if test else 100000\n",
    "\n",
    "\n",
    "# training the fake reviews model to an acceptable accuracy.\n",
    "# load the file\n",
    "review_path = 'reviews/YelpZip/reviewContent'\n",
    "meta_path = 'reviews/YelpZip/metadata'\n",
    "final_path = 'files/review_chunks/zip_cleaned_data_0'\n",
    "\n",
    "# actual col re-names and the masks were going to use\n",
    "review_cols = {0: 'user_id', 1: 'blank', 2: 'date', 3: 'text'}\n",
    "meta_cols = {0: 'user_id', 1: 'blank', 2: 'rating', 3: 'label', 4: 'date'}\n",
    "review_col_mask = ['text']\n",
    "meta_col_mask = ['rating', 'label']\n",
    "\n",
    "with open(review_path, 'r', encoding='utf-8') as f_review:\n",
    "    with open(meta_path, 'r', encoding='utf-8') as f_meta:\n",
    "        \n",
    "        print(f'Starting zipping')\n",
    "        t0 = time.time()\n",
    "        count = 1\n",
    "        chunk_r = []\n",
    "        chunk_m = []\n",
    "\n",
    "        for index, (line_f, line_m) in enumerate(zip(f_review, f_meta)):\n",
    "            \n",
    "            # read each line as a dataframe then append to a list\n",
    "            processed_line = convertLine(line_f)\n",
    "            chunk_r.append(processed_line)\n",
    "\n",
    "            processed_line = json.dumps(line_m.split('\\t'))\n",
    "            chunk_m.append(pd.read_json(StringIO(processed_line), lines=True))\n",
    "            \n",
    "            # save chunk to disk\n",
    "            if (index + 1) % chunk_size == 0:\n",
    "                chunk_r_df = pd.concat(chunk_r, ignore_index=True).rename(columns = review_cols)[review_col_mask]\n",
    "                chunk_m_df = pd.concat(chunk_m, ignore_index=True).rename(columns = meta_cols)[meta_col_mask]\n",
    "        \n",
    "                # remove the columns we don't care about and then concat them into the final data frame\n",
    "                # chunk_r_df = chunk_r_df[rwsub_less]\n",
    "                # chunk_m_df = chunk_m_df[]\n",
    "                final_df = pd.concat([chunk_r_df, chunk_m_df], axis=1)\n",
    "                final_df['rating'] = final_df['rating'] / 5.0\n",
    "\n",
    "                # write the cleaned and organized dataframe to a file\n",
    "                save_path = f'{final_path}{count}.csv'\n",
    "                final_df.to_csv(save_path, index=False)\n",
    "                \n",
    "                # clear out garbage\n",
    "                del chunk_r, chunk_m, chunk_r_df, chunk_m_df, final_df\n",
    "                gc.collect()\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f'Chunk {count} saved at {save_path}')\n",
    "                    print(f'Time elapsed: {time.time() - t0} seconds')\n",
    "                count += 1\n",
    "\n",
    "                chunk_r = []\n",
    "                chunk_m = []\n",
    "\n",
    "                if test and count > 3:\n",
    "                    break\n",
    "\n",
    "print('Finished zipping')\n",
    "if verbose:\n",
    "    print(f'Time elapsed: {time.time() - t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ef0d7-3dc4-40ab-acdd-584a2dbdf9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
