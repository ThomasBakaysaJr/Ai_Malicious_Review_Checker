{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef562cd8-dfb1-4ebf-8f46-8c50e7d68288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import time\n",
    "import gc\n",
    "from io import StringIO\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f3075d5-5083-4eda-8d8a-68b02251f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting zipping\n",
      "Chunk 1 saved at files/review_chunks/zip_cleaned_data_01.csv\n",
      "Time elapsed: 341.54082322120667 seconds\n",
      "Chunk 2 saved at files/review_chunks/zip_cleaned_data_02.csv\n",
      "Time elapsed: 702.17418384552 seconds\n",
      "Chunk 3 saved at files/review_chunks/zip_cleaned_data_03.csv\n",
      "Time elapsed: 1085.8003134727478 seconds\n",
      "Chunk 4 saved at files/review_chunks/zip_cleaned_data_04.csv\n",
      "Time elapsed: 1472.3731112480164 seconds\n",
      "Chunk 5 saved at files/review_chunks/zip_cleaned_data_05.csv\n",
      "Time elapsed: 1884.4284663200378 seconds\n",
      "Chunk 6 saved at files/review_chunks/zip_cleaned_data_06.csv\n",
      "Time elapsed: 2302.901432275772 seconds\n",
      "Finished zipping\n",
      "Time elapsed: 2333.0447268486023 seconds\n"
     ]
    }
   ],
   "source": [
    "# converts to lowercase and strip punctuation\n",
    "def convertLine(text, is_review):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + '/xa0'))\n",
    "\n",
    "    if is_review:\n",
    "        text = json.dumps(text.split('\\t', 3))\n",
    "    else:\n",
    "        text = json.dumps(text.split('\\t'))\n",
    "    text = pd.read_json(StringIO(text), lines=True)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# going to read it in chunks\n",
    "chunk_size = 100000\n",
    "\n",
    "\n",
    "# training the fake reviews model to an acceptable accuracy.\n",
    "# load the file\n",
    "review_path = 'reviews/YelpZip/reviewContent'\n",
    "meta_path = 'reviews/YelpZip/metadata'\n",
    "final_path = 'files/review_chunks/zip_cleaned_data_0'\n",
    "\n",
    "# actual col re-names and the masks were going to use\n",
    "review_cols = {0: 'user_id', 1: 'blank', 2: 'date', 3: 'text'}\n",
    "meta_cols = {0: 'user_id', 1: 'blank', 2: 'rating', 3: 'label', 4: 'date'}\n",
    "review_col_mask = ['text']\n",
    "meta_col_mask = ['rating', 'label']\n",
    "\n",
    "with open(review_path, 'r', encoding='utf-8') as f_review:\n",
    "    with open(meta_path, 'r', encoding='utf-8') as f_meta:\n",
    "        chunk_r = []\n",
    "        chunk_m = []\n",
    "\n",
    "        chunk_r_df = pd.DataFrame(columns = review_cols)\n",
    "        chunk_m_df = pd.DataFrame(columns = meta_cols)\n",
    "        \n",
    "        print(f'Starting zipping')\n",
    "        t0 = time.time()\n",
    "        count = 1\n",
    "        \n",
    "        for index, (line_f, line_m) in enumerate(zip(f_review, f_meta)):\n",
    "            # read each line as a dataframe then append to a list\n",
    "            # review structure [index,\n",
    "            chunk_r.append(convertLine(line_f, True))\n",
    "            chunk_m.append(convertLine(line_m, False))\n",
    "\n",
    "            # save chunk to disk\n",
    "            if (index + 1) % chunk_size == 0:\n",
    "                chunk_r_df = pd.concat(chunk_r, ignore_index=True).rename(columns = review_cols)[review_col_mask]\n",
    "                chunk_m_df = pd.concat(chunk_m, ignore_index=True).rename(columns = meta_cols)[meta_col_mask]\n",
    "        \n",
    "                # remove the columns we don't care about and then concat them into the final data frame\n",
    "                # chunk_r_df = chunk_r_df[rwsub_less]\n",
    "                # chunk_m_df = chunk_m_df[]\n",
    "                final_df = pd.concat([chunk_r_df, chunk_m_df], axis=1)\n",
    "                final_df['rating'] = final_df['rating'] / 5.0\n",
    "                final_df.convert_dtypes()\n",
    "        \n",
    "                # write the cleaned and organized dataframe to a file\n",
    "                save_path = f'{final_path}{count}.csv'\n",
    "                final_df.to_csv(save_path, index=False)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f'Chunk {count} saved at {save_path}')\n",
    "                    print(f'Time elapsed: {time.time() - t0} seconds')\n",
    "                count += 1\n",
    "\n",
    "print('Finished zipping')\n",
    "if verbose:\n",
    "    print(f'Time elapsed: {time.time() - t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "690b10b4-d6c7-422d-818e-b3a02deb81ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1435"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear out garbage\n",
    "del chunk_r, chunk_m, chunk_r_df, chunk_m_df, final_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842429d-3e9b-4c2f-8f74-ae6d482f2aa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorize with tf-idf\n",
    "vizer = TfidfVectorizer()\n",
    "\n",
    "x_text = vizer.fit_transform(fakeDf['text_'])\n",
    "\n",
    "if verbose:\n",
    "    print(x_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31cd57-bf3e-4aaf-9347-d07679a248c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the sparse matrix with the dense ratings column\n",
    "\n",
    "# turn into 2d array\n",
    "rate_feature = fakeDf['rating'].values.reshape(-1, 1)\n",
    "\n",
    "# combine vectorized text and ratings\n",
    "# data\n",
    "X = hstack([x_text, rate_feature])\n",
    "\n",
    "# target labels\n",
    "y = fakeDf['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120425e4-6867-40b9-97fb-905f4b875440",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "c_params = [1.0, 0.7, 0.5, 0.3]\n",
    "clf_names = []\n",
    "\n",
    "# train the thing || trying differnt C score on SVM and Logistic Regression\n",
    "models = {}\n",
    "\n",
    "for c in c_params:\n",
    "    # create a svm models with this c value\n",
    "    name = 'clf_svm' + str(c)\n",
    "    clf_names.append(name)\n",
    "    models[name] = LinearSVC(C=c)\n",
    "\n",
    "    t0 = time.time()\n",
    "    if verbose:\n",
    "        print(f'Training {name}')\n",
    "    # actual training\n",
    "    models[name].fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print(f'finished after {time.time() - t0} seconds')\n",
    "\n",
    "    # create a logistic regression models with this c value\n",
    "    name = 'clf_log' + str(c)\n",
    "    clf_names.append(name)\n",
    "    models[name] = LogisticRegression(C=c, max_iter=1000)\n",
    "\n",
    "    t0 = time.time()\n",
    "    if verbose:\n",
    "        print(f'Training {name}')\n",
    "    # actual training\n",
    "    models[name].fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print(f'finished after {time.time() - t0} seconds')\n",
    "\n",
    "# train a single random forest classifier\n",
    "name = 'clf_for'\n",
    "clf_names.append(name)\n",
    "t0 = time.time()\n",
    "if verbose:\n",
    "    print(f'Training {name}')\n",
    "models[name] = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "models[name].fit(X_train, y_train)\n",
    "if verbose:\n",
    "    print(f'finished after {time.time() - t0} seconds')\n",
    "\n",
    "if verbose:\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf43cd3-5cdd-4142-96dc-9c115aef02df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in clf_names:\n",
    "    y_pred = models[name].predict(X_test)\n",
    "    print(f'Classification report for {name}')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc89f5c-8bf9-4971-9cd3-34c68d5a07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing really beat the SVM with C=1.0\n",
    "# going to save that one and the vectorized (tf-idf)\n",
    "# important because the model trained on this\n",
    "# any new words might give me garbage.\n",
    "joblib.dump(models['clf_svm1.0'], 'models/support_svm.pkl')\n",
    "joblib.dump(models['clf_log1.0'], 'models/support_log.pkl')\n",
    "joblib.dump(models['clf_for'], 'models/support_for.pkl')\n",
    "joblib.dump(vizer, 'models/support_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42fa3e2-17bb-4d19-a208-7f39d6778d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac6491-ace6-4a73-8a44-02b0431bac9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
