{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ca8304c-93cb-4cb1-873b-7d804543992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to clean the reviews here.\n",
    "import ast\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import autopep8\n",
    "import joblib\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy.sparse import hstack\n",
    "from io import StringIO\n",
    "\n",
    "# do get rid of annoying warnings\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8862ce46-fcb1-4da2-9c7c-21a38f524c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global, for the print statements\n",
    "verbose = True\n",
    "# set to True so that the notebook tries smaller chunks and only does 5 chunks\n",
    "test = False\n",
    "# How confident does the model need to be to accept the psuedo-label\n",
    "threshold = 0.8\n",
    "# chunk_size (will only use this if test if False)\n",
    "CHUNK_SIZE = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0acc26d8-89a1-41ad-84fc-8cc8cb6d7236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap model and vectorizer loaded.\n"
     ]
    }
   ],
   "source": [
    "vizer_path = 'models/support_vectorizer.pkl'\n",
    "bootstrap_model_path = 'models/support_log.pkl'\n",
    "\n",
    "# load the tf-idf vectorizer and support_svm\n",
    "try:\n",
    "    vizer = joblib.load(vizer_path)\n",
    "    bootstrap_model = joblib.load(bootstrap_model_path)\n",
    "    print('Bootstrap model and vectorizer loaded.')\n",
    "except Exception as e:\n",
    "    print(f\"Can't load bootstrap model and vectorizer due to: {e}\")\n",
    "    print(f'Cannot continue without those.')\n",
    "    sys.exit\n",
    "    \n",
    "# this should probably be on a seperate cell so I don't constantly reload the dataframes\n",
    "fDefPath = 'reviews/yelpReviews/yelp_academic_dataset_'\n",
    "# constants so I don't have to keep changing names\n",
    "BS = 'business'\n",
    "CH = 'checkin'\n",
    "TI = 'tip'\n",
    "RW = 'review'\n",
    "US = 'user'\n",
    "\n",
    "# subsets of what i care about\n",
    "bssub = ['business_id', 'postal_code',\n",
    "         'review_count', 'attributes', 'categories']\n",
    "ussub = ['user_id', 'review_count', 'yelping_since']\n",
    "# this top one is for when we use for final training\n",
    "rwsub = ['user_id', 'business_id', 'stars', 'text', 'date']\n",
    "# for bootstrap model, we will need to link the user_ids to this for\n",
    "# when training the bigger model. For actual bootstrap training we'll\n",
    "# stip the user out of it\n",
    "rwsub_less = ['user_id','stars', 'text']\n",
    "\n",
    "# constants for the file path\n",
    "bspath = f'{fDefPath}{BS}.json'\n",
    "chpath = f'{fDefPath}{CH}.json'\n",
    "tipath = f'{fDefPath}{TI}.json'\n",
    "rwpath = f'{fDefPath}{RW}.json'\n",
    "uspath = f'{fDefPath}{US}.json'\n",
    "\n",
    "chunk_save_path = 'files/review_chunks/review_chunk_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e40587d-71b2-4883-9ffd-8576b4bf13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and then predict. returns predictions\n",
    "def clean_predict(in_chunk_df):\n",
    "    in_chunk_df = in_chunk_df\n",
    "    # clean text and then normalize rating\n",
    "    in_chunk_df['text'] = in_chunk_df['text'].apply(clean_text)\n",
    "    in_chunk_df['stars'] = in_chunk_df['stars'] / 5.0\n",
    "\n",
    "    # vectorize chunk's text\n",
    "    x_text = vizer.transform(in_chunk_df['text'])\n",
    "    # convert starts (rating) to 2d array\n",
    "    rate_feature = in_chunk_df['stars'].values.reshape(-1,1)\n",
    "    \n",
    "    # crate the hstack to be used in the support model\n",
    "    X = hstack([x_text, rate_feature])\n",
    "    \n",
    "    # return predictions based on 'confidence' scores\n",
    "    # 0 means the model is confident (above threshold) that it's a real review\n",
    "    # 1 means \n",
    "    conf_scores = bootstrap_model.predict_proba(X)\n",
    "    real_mask = np.where((conf_scores[:, 0] > threshold), 0, 2)\n",
    "    fake_mask = np.where((conf_scores[:, 1] > threshold), 1, 2)\n",
    "\n",
    "    y_pred = pd.DataFrame(np.where(real_mask == 0, 0, np.where(fake_mask == 1, 1, 2)))\n",
    "    # rename column\n",
    "    if verbose:\n",
    "        print(f'Values in this chunk.\\n{y_pred.value_counts()}')\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# converts to lowercase and strip punctuation\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85b822ed-1268-47ae-a771-cde00c8f65ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting labeling\n",
      "Values in this chunk.\n",
      "0\n",
      "2    178\n",
      "0     16\n",
      "1      6\n",
      "Name: count, dtype: int64\n",
      "chunk 1 finished. Saved at files/review_chunks/review_chunk_01.csv\n",
      "\n",
      "Values in this chunk.\n",
      "0\n",
      "2    186\n",
      "0      7\n",
      "1      7\n",
      "Name: count, dtype: int64\n",
      "chunk 2 finished. Saved at files/review_chunks/review_chunk_02.csv\n",
      "\n",
      "Values in this chunk.\n",
      "0\n",
      "2    182\n",
      "1     10\n",
      "0      8\n",
      "Name: count, dtype: int64\n",
      "chunk 3 finished. Saved at files/review_chunks/review_chunk_03.csv\n",
      "\n",
      "Values in this chunk.\n",
      "0\n",
      "2    182\n",
      "0     13\n",
      "1      5\n",
      "Name: count, dtype: int64\n",
      "chunk 4 finished. Saved at files/review_chunks/review_chunk_04.csv\n",
      "\n",
      "Values in this chunk.\n",
      "0\n",
      "2    182\n",
      "0     10\n",
      "1      8\n",
      "Name: count, dtype: int64\n",
      "chunk 5 finished. Saved at files/review_chunks/review_chunk_05.csv\n",
      "\n",
      "TEST RUN\n",
      "Finished labeling reviews after 0.06276732285817464 minutes. Files are seperated into chunks of 200 lines.\n"
     ]
    }
   ],
   "source": [
    "# going to have read the json file in chunks, the thing is almost 5 gigs\n",
    "chunk_size = 200 if test else CHUNK_SIZE\n",
    "\n",
    "with open(rwpath, 'r', encoding='utf-8') as file:\n",
    "    chunk = []\n",
    "    count = 1\n",
    "    print(f'Starting labeling')\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for index, line in enumerate(file):\n",
    "        # read each line as a dataframe then append to a list\n",
    "        data = pd.read_json(StringIO(line), lines = True)\n",
    "        chunk.append(data)\n",
    "\n",
    "        # check if chunk is full / right now we exit since I'm just trying to clean the thing rn.\n",
    "        if (index + 1) % chunk_size == 0:\n",
    "            #print(chunk)\n",
    "            chunk_df = pd.concat(chunk, ignore_index=True)\n",
    "            \n",
    "            # remove the columns we don't care about\n",
    "            chunk_df = chunk_df[rwsub_less]\n",
    "\n",
    "            # predict pseudo labels\n",
    "            chunk_df = pd.concat([chunk_df, clean_predict(chunk_df)], axis = 1)\n",
    "            chunk_df.rename({0:'pseudo_label'}, inplace = True, axis='columns')\n",
    "\n",
    "            # remove entries that the model was not confident on (true or fake)\n",
    "            chunk_df = chunk_df[chunk_df['pseudo_label'] != 2]\n",
    "            \n",
    "            # write each chunk to its own file, will combine them later\n",
    "            chunk_path = f'{chunk_save_path}{count}.csv'\n",
    "            chunk_df.to_csv(chunk_path, index=False)\n",
    "\n",
    "            if verbose:\n",
    "                print(f'chunk {count} finished. Saved at {chunk_path}\\n')\n",
    "                \n",
    "            # garbage collection\n",
    "            del chunk, chunk_df\n",
    "            gc.collect()\n",
    "\n",
    "            chunk = []\n",
    "                \n",
    "            count += 1\n",
    "            if test and count > 5:\n",
    "                break\n",
    "if test:\n",
    "    print('TEST RUN')\n",
    "print(f'Finished labeling reviews after {(time.time() - t0) / 60.0} minutes. Files are seperated into chunks of {chunk_size} lines.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1d189-a45f-4931-823f-031cb71f2439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
