{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca8304c-93cb-4cb1-873b-7d804543992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to clean the reviews here.\n",
    "import ast\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import autopep8\n",
    "import joblib\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy.sparse import hstack\n",
    "from io import StringIO\n",
    "\n",
    "# do get rid of annoying warnings\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8862ce46-fcb1-4da2-9c7c-21a38f524c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global, for the print statements\n",
    "verbose = True\n",
    "# set to True so that the notebook tries smaller chunks and only does 5 chunks\n",
    "test = False\n",
    "# How confident does the model need to be to accept the psuedo-label\n",
    "threshold = 0.8\n",
    "# chunk_size (will only use this if test if False)\n",
    "CHUNK_SIZE = 100000\n",
    "\n",
    "chunk_save_path = f\"files/review_chunks/review_labeling/review_chunk{'_test' if test else ''}_0\"\n",
    "csv_save_path = 'files/reviews_pseudo_labeled.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0acc26d8-89a1-41ad-84fc-8cc8cb6d7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizer_path = 'models/support_vectorizer.pkl'\n",
    "bootstrap_model_path = 'models/support_log.pkl'\n",
    "    \n",
    "# this should probably be on a seperate cell so I don't constantly reload the dataframes\n",
    "fDefPath = 'reviews/yelpReviews/yelp_academic_dataset_'\n",
    "# constants so I don't have to keep changing names\n",
    "BS = 'business'\n",
    "CH = 'checkin'\n",
    "TI = 'tip'\n",
    "RW = 'review'\n",
    "US = 'user'\n",
    "\n",
    "# subsets of what i care about\n",
    "bssub = ['business_id', 'postal_code',\n",
    "         'review_count', 'attributes', 'categories']\n",
    "ussub = ['user_id', 'review_count', 'yelping_since']\n",
    "# this top one is for when we use for final training\n",
    "rwsub = ['user_id', 'business_id', 'stars', 'text', 'date']\n",
    "# for bootstrap model, we will need to link the user_ids to this for\n",
    "# when training the bigger model. For actual bootstrap training we'll\n",
    "# stip the user out of it\n",
    "rwsub_less = ['user_id','business_id','stars', 'text']\n",
    "\n",
    "# constants for the file path\n",
    "bspath = f'{fDefPath}{BS}.json'\n",
    "chpath = f'{fDefPath}{CH}.json'\n",
    "tipath = f'{fDefPath}{TI}.json'\n",
    "rwpath = f'{fDefPath}{RW}.json'\n",
    "uspath = f'{fDefPath}{US}.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e40587d-71b2-4883-9ffd-8576b4bf13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and then predict. returns predictions\n",
    "def clean_predict(in_chunk_df):\n",
    "    in_chunk_df = in_chunk_df\n",
    "    # clean text and then normalize rating\n",
    "    in_chunk_df['text'] = in_chunk_df['text'].apply(clean_text)\n",
    "    in_chunk_df['stars'] = in_chunk_df['stars'] / 5.0\n",
    "\n",
    "    # vectorize chunk's text\n",
    "    x_text = vizer.transform(in_chunk_df['text'])\n",
    "    # convert starts (rating) to 2d array\n",
    "    rate_feature = in_chunk_df['stars'].values.reshape(-1,1)\n",
    "    \n",
    "    # crate the hstack to be used in the support model\n",
    "    X = hstack([x_text, rate_feature])\n",
    "    \n",
    "    # return predictions based on 'confidence' scores\n",
    "    # 0 means the model is confident (above threshold) that it's a real review\n",
    "    # 1 means \n",
    "    conf_scores = bootstrap_model.predict_proba(X)\n",
    "    real_mask = np.where((conf_scores[:, 0] > threshold), 0, 2)\n",
    "    fake_mask = np.where((conf_scores[:, 1] > threshold), 1, 2)\n",
    "\n",
    "    y_pred = pd.DataFrame(np.where(real_mask == 0, 0, np.where(fake_mask == 1, 1, 2)))\n",
    "    # rename column\n",
    "    if verbose:\n",
    "        print(f'Values in this chunk.\\n{y_pred.value_counts()}')\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# converts to lowercase and strip punctuation\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b822ed-1268-47ae-a771-cde00c8f65ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap model and vectorizer loaded.\n",
      "Starting labeling\n",
      "Chunk 1 already exits at files/review_chunks/review_labeling/review_chunk_01.csv. Skipping\n",
      "Chunk 2 already exits at files/review_chunks/review_labeling/review_chunk_02.csv. Skipping\n",
      "Chunk 3 already exits at files/review_chunks/review_labeling/review_chunk_03.csv. Skipping\n",
      "Chunk 4 already exits at files/review_chunks/review_labeling/review_chunk_04.csv. Skipping\n",
      "Chunk 5 already exits at files/review_chunks/review_labeling/review_chunk_05.csv. Skipping\n",
      "Chunk 6 already exits at files/review_chunks/review_labeling/review_chunk_06.csv. Skipping\n",
      "Chunk 7 already exits at files/review_chunks/review_labeling/review_chunk_07.csv. Skipping\n",
      "Chunk 8 already exits at files/review_chunks/review_labeling/review_chunk_08.csv. Skipping\n",
      "Chunk 9 already exits at files/review_chunks/review_labeling/review_chunk_09.csv. Skipping\n",
      "Chunk 10 already exits at files/review_chunks/review_labeling/review_chunk_010.csv. Skipping\n",
      "Chunk 11 already exits at files/review_chunks/review_labeling/review_chunk_011.csv. Skipping\n",
      "Chunk 12 already exits at files/review_chunks/review_labeling/review_chunk_012.csv. Skipping\n",
      "Chunk 13 already exits at files/review_chunks/review_labeling/review_chunk_013.csv. Skipping\n",
      "Chunk 14 already exits at files/review_chunks/review_labeling/review_chunk_014.csv. Skipping\n",
      "Chunk 15 already exits at files/review_chunks/review_labeling/review_chunk_015.csv. Skipping\n",
      "Chunk 16 already exits at files/review_chunks/review_labeling/review_chunk_016.csv. Skipping\n",
      "Chunk 17 already exits at files/review_chunks/review_labeling/review_chunk_017.csv. Skipping\n",
      "Chunk 18 already exits at files/review_chunks/review_labeling/review_chunk_018.csv. Skipping\n",
      "Chunk 19 already exits at files/review_chunks/review_labeling/review_chunk_019.csv. Skipping\n",
      "Chunk 20 already exits at files/review_chunks/review_labeling/review_chunk_020.csv. Skipping\n",
      "Chunk 21 already exits at files/review_chunks/review_labeling/review_chunk_021.csv. Skipping\n",
      "Chunk 22 already exits at files/review_chunks/review_labeling/review_chunk_022.csv. Skipping\n",
      "Chunk 23 already exits at files/review_chunks/review_labeling/review_chunk_023.csv. Skipping\n",
      "Chunk 24 already exits at files/review_chunks/review_labeling/review_chunk_024.csv. Skipping\n",
      "Chunk 25 already exits at files/review_chunks/review_labeling/review_chunk_025.csv. Skipping\n",
      "Chunk 26 already exits at files/review_chunks/review_labeling/review_chunk_026.csv. Skipping\n",
      "Chunk 27 already exits at files/review_chunks/review_labeling/review_chunk_027.csv. Skipping\n",
      "Chunk 28 already exits at files/review_chunks/review_labeling/review_chunk_028.csv. Skipping\n",
      "Chunk 29 already exits at files/review_chunks/review_labeling/review_chunk_029.csv. Skipping\n",
      "Chunk 30 already exits at files/review_chunks/review_labeling/review_chunk_030.csv. Skipping\n",
      "Chunk 31 already exits at files/review_chunks/review_labeling/review_chunk_031.csv. Skipping\n",
      "Chunk 32 already exits at files/review_chunks/review_labeling/review_chunk_032.csv. Skipping\n",
      "Chunk 33 already exits at files/review_chunks/review_labeling/review_chunk_033.csv. Skipping\n",
      "Chunk 34 already exits at files/review_chunks/review_labeling/review_chunk_034.csv. Skipping\n",
      "Chunk 35 already exits at files/review_chunks/review_labeling/review_chunk_035.csv. Skipping\n",
      "Chunk 36 already exits at files/review_chunks/review_labeling/review_chunk_036.csv. Skipping\n",
      "Chunk 37 already exits at files/review_chunks/review_labeling/review_chunk_037.csv. Skipping\n",
      "Chunk 38 already exits at files/review_chunks/review_labeling/review_chunk_038.csv. Skipping\n",
      "Chunk 39 already exits at files/review_chunks/review_labeling/review_chunk_039.csv. Skipping\n",
      "Chunk 40 already exits at files/review_chunks/review_labeling/review_chunk_040.csv. Skipping\n",
      "Chunk 41 already exits at files/review_chunks/review_labeling/review_chunk_041.csv. Skipping\n",
      "Chunk 42 already exits at files/review_chunks/review_labeling/review_chunk_042.csv. Skipping\n",
      "Chunk 43 already exits at files/review_chunks/review_labeling/review_chunk_043.csv. Skipping\n",
      "Chunk 44 already exits at files/review_chunks/review_labeling/review_chunk_044.csv. Skipping\n",
      "Chunk 45 already exits at files/review_chunks/review_labeling/review_chunk_045.csv. Skipping\n",
      "Chunk 46 already exits at files/review_chunks/review_labeling/review_chunk_046.csv. Skipping\n",
      "Chunk 47 already exits at files/review_chunks/review_labeling/review_chunk_047.csv. Skipping\n",
      "Chunk 48 already exits at files/review_chunks/review_labeling/review_chunk_048.csv. Skipping\n",
      "Chunk 49 already exits at files/review_chunks/review_labeling/review_chunk_049.csv. Skipping\n",
      "Chunk 50 already exits at files/review_chunks/review_labeling/review_chunk_050.csv. Skipping\n",
      "Chunk 51 already exits at files/review_chunks/review_labeling/review_chunk_051.csv. Skipping\n",
      "Chunk 52 already exits at files/review_chunks/review_labeling/review_chunk_052.csv. Skipping\n",
      "Chunk 53 already exits at files/review_chunks/review_labeling/review_chunk_053.csv. Skipping\n",
      "Chunk 54 already exits at files/review_chunks/review_labeling/review_chunk_054.csv. Skipping\n",
      "Chunk 55 already exits at files/review_chunks/review_labeling/review_chunk_055.csv. Skipping\n",
      "Chunk 56 already exits at files/review_chunks/review_labeling/review_chunk_056.csv. Skipping\n",
      "Chunk 57 already exits at files/review_chunks/review_labeling/review_chunk_057.csv. Skipping\n",
      "Chunk 58 already exits at files/review_chunks/review_labeling/review_chunk_058.csv. Skipping\n",
      "Chunk 59 already exits at files/review_chunks/review_labeling/review_chunk_059.csv. Skipping\n",
      "Chunk 60 already exits at files/review_chunks/review_labeling/review_chunk_060.csv. Skipping\n",
      "Chunk 61 already exits at files/review_chunks/review_labeling/review_chunk_061.csv. Skipping\n",
      "Chunk 62 already exits at files/review_chunks/review_labeling/review_chunk_062.csv. Skipping\n",
      "Chunk 63 already exits at files/review_chunks/review_labeling/review_chunk_063.csv. Skipping\n",
      "Chunk 64 already exits at files/review_chunks/review_labeling/review_chunk_064.csv. Skipping\n",
      "Chunk 65 already exits at files/review_chunks/review_labeling/review_chunk_065.csv. Skipping\n",
      "Chunk 66 already exits at files/review_chunks/review_labeling/review_chunk_066.csv. Skipping\n",
      "Chunk 67 already exits at files/review_chunks/review_labeling/review_chunk_067.csv. Skipping\n",
      "Chunk 68 already exits at files/review_chunks/review_labeling/review_chunk_068.csv. Skipping\n",
      "Chunk 69 already exits at files/review_chunks/review_labeling/review_chunk_069.csv. Skipping\n",
      "Finished labeling reviews after 0.2185459812482198 minutes. Files are seperated into chunks of 100000 lines.\n",
      "Combining chunks\n",
      "Combined. Cleaned review files lives at files/reviews_pseudo_labeled.csv\n"
     ]
    }
   ],
   "source": [
    "# going to have read the json file in chunks, the thing is almost 5 gigs\n",
    "chunk_size = 200 if test else CHUNK_SIZE\n",
    "\n",
    "# load the tf-idf vectorizer and support_svm\n",
    "try:\n",
    "    vizer = joblib.load(vizer_path)\n",
    "    bootstrap_model = joblib.load(bootstrap_model_path)\n",
    "    print('Bootstrap model and vectorizer loaded.')\n",
    "except Exception as e:\n",
    "    print(f\"Can't load bootstrap model and vectorizer due to: {e}\")\n",
    "    print(f'Cannot continue without those.')\n",
    "    sys.exit\n",
    "\n",
    "with open(rwpath, 'r', encoding='utf-8') as file:\n",
    "        chunk = []\n",
    "        chunk_df = pd.DataFrame()\n",
    "        chunk_count = []\n",
    "        count = 1\n",
    "        print(f'Starting labeling')\n",
    "        t0 = time.time()\n",
    "        # check if this chunk exists already (for restarts)\n",
    "        chunk_path = f'{chunk_save_path}{count}.csv'\n",
    "        \n",
    "        if os.path.exists(chunk_path):\n",
    "            chunk_count.append(count)\n",
    "        \n",
    "        for index, line in enumerate(file):            \n",
    "            if count not in chunk_count:\n",
    "                # read each line as a dataframe then append to a list\n",
    "                data = json.loads(line)\n",
    "                #data = pd.read_json(StringIO(line), lines = True)\n",
    "                chunk.append(data)\n",
    "            \n",
    "            # check if chunk is full / right now we exit since I'm just trying to clean the thing rn.\n",
    "            if (index + 1) % chunk_size == 0:\n",
    "                if os.path.exists(chunk_path):\n",
    "                    print(f'Chunk {count} already exits at {chunk_path}. Skipping')\n",
    "                else:\n",
    "                    print(f'Starting labeling of chunk {count}.')\n",
    "                    chunk_df = pd.DataFrame(chunk)\n",
    "                    \n",
    "                    # remove the columns we don't care about\n",
    "                    chunk_df = chunk_df[rwsub_less]\n",
    "        \n",
    "                    # predict pseudo labels\n",
    "                    chunk_df = pd.concat([chunk_df, clean_predict(chunk_df)], axis = 1)\n",
    "                    chunk_df.rename({0:'pseudo_label'}, inplace = True, axis='columns')\n",
    "        \n",
    "                    # remove entries that the model was not confident on (true or fake)\n",
    "                    chunk_df = chunk_df[chunk_df['pseudo_label'] != 2]\n",
    "                    \n",
    "                    # write each chunk to its own file, will combine them later\n",
    "                    chunk_df.to_csv(chunk_path, index=False)\n",
    "        \n",
    "                    if verbose:\n",
    "                        print(f'chunk {count} finished at {time.time() - t0} seconds. Saved at {chunk_path}\\n')\n",
    "                        \n",
    "                # garbage collection\n",
    "                del chunk, chunk_df\n",
    "                gc.collect()\n",
    "        \n",
    "                chunk = []\n",
    "                chunk_df = pd.DataFrame()\n",
    "                \n",
    "                if count not in chunk_count:\n",
    "                    chunk_count.append(count)\n",
    "                count += 1\n",
    "                chunk_path = f'{chunk_save_path}{count}.csv'\n",
    "                \n",
    "                if os.path.exists(chunk_path):\n",
    "                    chunk_count.append(count)\n",
    "                \n",
    "                if test and count > 5:\n",
    "                    break\n",
    "if test:\n",
    "    print('TEST RUN')\n",
    "print(f'Finished labeling reviews after {(time.time() - t0) / 60.0} minutes. Files are seperated into chunks of {chunk_size} lines.')\n",
    "\n",
    "print('Combining chunks')\n",
    "chunk_files = sorted(glob.glob(f'{chunk_save_path}*.csv'))\n",
    "df = pd.concat([pd.read_csv(file) for file in chunk_files], ignore_index = True)\n",
    "df.to_csv(csv_save_path, index=False)\n",
    "print(f'Combined. Cleaned review files lives at {csv_save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1d189-a45f-4931-823f-031cb71f2439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb02ea-8078-4953-8fb1-839f11eb3614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
