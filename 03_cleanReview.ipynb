{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca8304c-93cb-4cb1-873b-7d804543992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to clean the reviews here.\n",
    "import ast\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import autopep8\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy.sparse import hstack\n",
    "from io import StringIO\n",
    "\n",
    "# do get rid of annoying warnings\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0acc26d8-89a1-41ad-84fc-8cc8cb6d7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global, for the print statements\n",
    "verbose = True\n",
    "\n",
    "# load the tf-idf vectorizer and support_svm\n",
    "vizer = joblib.load('models/support_vectorizer.pkl')\n",
    "\n",
    "support_svm = joblib.load('models/support_svm.pkl')\n",
    "support_log = joblib.load('models/support_log.pkl')\n",
    "support_for = joblib.load('models/support_for.pkl')\n",
    "support_models = {\n",
    "    'svm' : support_svm,\n",
    "    'log' : support_log,\n",
    "    'for' : support_for\n",
    "} \n",
    "\n",
    "# this should probably be on a seperate cell so I don't constantly reload the dataframes\n",
    "fDefPath = 'reviews/yelpReviews/yelp_academic_dataset_'\n",
    "# constants so I don't have to keep changing names\n",
    "BS = 'business'\n",
    "CH = 'checkin'\n",
    "TI = 'tip'\n",
    "RW = 'review'\n",
    "US = 'user'\n",
    "\n",
    "# subsets of what i care about\n",
    "bssub = ['business_id', 'postal_code',\n",
    "         'review_count', 'attributes', 'categories']\n",
    "ussub = ['user_id', 'review_count', 'yelping_since']\n",
    "# this top one is for when we use for final training\n",
    "rwsub = ['user_id', 'business_id', 'stars', 'text', 'date']\n",
    "rwsub_less = ['stars', 'text']\n",
    "\n",
    "# constants for the file path\n",
    "bspath = f'{fDefPath}{BS}.json'\n",
    "chpath = f'{fDefPath}{CH}.json'\n",
    "tipath = f'{fDefPath}{TI}.json'\n",
    "rwpath = f'{fDefPath}{RW}.json'\n",
    "uspath = f'{fDefPath}{US}.json'\n",
    "\n",
    "chunk_save_path = 'files/review_chunks/review_chunk_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e40587d-71b2-4883-9ffd-8576b4bf13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and then predict. returns predictions\n",
    "def clean_predict(in_chunk_df):\n",
    "    in_chunk_df = in_chunk_df\n",
    "    # clean text and then normalize rating\n",
    "    in_chunk_df['text'] = in_chunk_df['text'].apply(clean_text)\n",
    "    in_chunk_df['stars'] = in_chunk_df['stars'] / 5.0\n",
    "\n",
    "    # vectorize chunk's text\n",
    "    x_text = vizer.transform(in_chunk_df['text'])\n",
    "    # convert starts (rating) to 2d array\n",
    "    rate_feature = in_chunk_df['stars'].values.reshape(-1,1)\n",
    "    \n",
    "    # crate the hstack to be used in the support model\n",
    "    X = hstack([x_text, rate_feature])\n",
    "\n",
    "    y_pred = pd.DataFrame()\n",
    "\n",
    "    for model_name in support_models.keys():\n",
    "        # return predictions\n",
    "        if model_name == 'svm':\n",
    "            conf_scores = support_models[model_name].decision_function(X)\n",
    "            mask = conf_scores > 0.8\n",
    "        else:\n",
    "            conf_scores = support_models[model_name].predict_proba(X)\n",
    "            mask = conf_scores[:, 1] > 0.8\n",
    "            #print(mask)\n",
    "\n",
    "        final_mask = np.where(mask, 1, 0)\n",
    "        pred = pd.DataFrame(final_mask, columns=[model_name])\n",
    "        y_pred = pd.concat([y_pred, pred], axis = 1)\n",
    "\n",
    "    if verbose:\n",
    "        print (len(y_pred))\n",
    "        print (in_chunk_df.shape)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# converts to lowercase and strip punctuation\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b822ed-1268-47ae-a771-cde00c8f65ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting labeling\n",
      "10000\n",
      "(10000, 2)\n",
      "chunk 1 finished\n",
      "Finished labeling reviews after 0.563196583588918 minutes. Written as files/review_chunks/review_chunk_0number, will need to combined later.\n"
     ]
    }
   ],
   "source": [
    "# going to have read the json file in chunks, the thing is almost 5 gigs\n",
    "chunk_size = 10000\n",
    "\n",
    "with open(rwpath, 'r', encoding='utf-8') as file:\n",
    "    chunk = []\n",
    "    count = 1\n",
    "    print(f'Starting labeling')\n",
    "    t0 = time.time()\n",
    "    for index, line in enumerate(file):\n",
    "        # read each line as a dataframe then append to a list\n",
    "        data = pd.read_json(StringIO(line), lines = True)\n",
    "        chunk.append(data)\n",
    "\n",
    "        # check if chunk is full / right now we exit since I'm just trying to clean the thing rn.\n",
    "        if (index + 1) % chunk_size == 0:\n",
    "            #print(chunk)\n",
    "            chunk_df = pd.concat(chunk, ignore_index=True)\n",
    "            \n",
    "            # remove the columns we don't care about\n",
    "            chunk_df = chunk_df[rwsub_less]\n",
    "            chunk_df = pd.concat([chunk_df, clean_predict(chunk_df)], axis = 1)\n",
    "            \n",
    "            # write each chunk to its own file, will combine them later\n",
    "            chunk_path = f'{chunk_save_path}{count}.csv'\n",
    "            chunk_df.to_csv(chunk_path, index=False)\n",
    "\n",
    "            if verbose:\n",
    "                print(f'chunk {count} finished')\n",
    "                \n",
    "            count += 1\n",
    "            break\n",
    "            \n",
    "print(f'Finished labeling reviews after {(time.time() - t0) / 60.0} minutes. Written as {chunk_save_path}number, will need to combined later.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b9d936-1445-4210-b05a-0dca5d27543f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d3678-cd00-4dd3-866e-b4fc0175c1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9bb75c-a7d1-4191-ac4d-4626d055af42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
